#!/usr/bin/env python3
"""
Test Phase 3 SimplifiÃ© - SuperWhisper2 Engine V5
Test infrastructure Phase 3 sans dÃ©pendances audio complexes
Focus: INT8 Quantification + GPU RTX 3090 + Infrastructure V5
"""

import os
import sys
import time
import logging
from datetime import datetime

# Forcer utilisation RTX 3090 24GB (GPU 1)
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'

# Test imports Phase 3
try:
    import torch
    import torch.cuda
    from faster_whisper import WhisperModel
    import transformers
    import accelerate
    import optimum
    
    PHASE3_DEPS_OK = True
    print("âœ… DÃ©pendances Phase 3 installÃ©es:")
    print(f"   - torch: {torch.__version__}")
    print(f"   - transformers: {transformers.__version__}")
    print(f"   - CUDA disponible: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"   - GPU: {gpu_name} ({gpu_memory:.1f}GB)")
    
except ImportError as e:
    PHASE3_DEPS_OK = False
    print(f"âŒ DÃ©pendances Phase 3 manquantes: {e}")


class SimpleModelOptimizer:
    """
    Version simplifiÃ©e de ModelOptimizer pour test infrastructure
    """
    
    def __init__(self, logger):
        self.logger = logger
        self.int8_model = None
        self.fp16_model = None
        self.quantization_enabled = False
        
    def test_int8_quantification(self):
        """
        Test simplifiÃ© INT8 quantification
        """
        try:
            self.logger.info("ğŸ”§ Test quantification INT8...")
            
            if not torch.cuda.is_available():
                self.logger.error("âŒ CUDA non disponible")
                return False
            
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            self.logger.info(f"ğŸ® GPU VRAM: {gpu_memory:.1f}GB")
            
            # Test crÃ©ation modÃ¨le Whisper
            self.logger.info("ğŸ“¦ Chargement modÃ¨le Whisper medium...")
            
            # ModÃ¨le FP16 d'abord
            start_time = time.time()
            self.fp16_model = WhisperModel(
                "medium",
                device="cuda", 
                compute_type="float16"
            )
            fp16_load_time = time.time() - start_time
            self.logger.info(f"âœ… ModÃ¨le FP16 chargÃ© en {fp16_load_time:.1f}s")
            
            # Test INT8 si GPU suffisant
            if gpu_memory >= 8:  # Minimum 8GB pour INT8
                start_time = time.time()
                self.int8_model = WhisperModel(
                    "medium",
                    device="cuda",
                    compute_type="int8"
                )
                int8_load_time = time.time() - start_time
                self.logger.info(f"âœ… ModÃ¨le INT8 chargÃ© en {int8_load_time:.1f}s")
                
                self.quantization_enabled = True
                return True
            else:
                self.logger.warning(f"âš ï¸ GPU {gpu_memory:.1f}GB insuffisant pour INT8")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ Erreur test INT8: {e}")
            return False


class SimplePhase3Engine:
    """
    Version simplifiÃ©e de SuperWhisper2EngineV5 pour test infrastructure
    """
    
    def __init__(self):
        self.logger = self._setup_logging()
        self.model_optimizer = None
        self.phase3_optimizations = {
            'int8_quantization': False,
            'gpu_memory_available': False,
            'cuda_streams': False,
            'vram_cache': False
        }
        
    def _setup_logging(self):
        """Setup logging"""
        logger = logging.getLogger('SimplePhase3Engine')
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - ENGINE - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def test_phase3_infrastructure(self):
        """
        Test infrastructure Phase 3 complÃ¨te
        """
        self.logger.info("ğŸš€ DÃ‰BUT TEST INFRASTRUCTURE PHASE 3")
        
        results = {
            'timestamp': datetime.now().isoformat(),
            'tests': {},
            'overall_success': False
        }
        
        # Test 1: GPU et CUDA
        gpu_test = self._test_gpu_infrastructure()
        results['tests']['gpu_infrastructure'] = gpu_test
        self.phase3_optimizations['gpu_memory_available'] = gpu_test['success']
        
        # Test 2: Model Optimizer
        if gpu_test['success']:
            model_test = self._test_model_optimizer()
            results['tests']['model_optimizer'] = model_test
            self.phase3_optimizations['int8_quantization'] = model_test['success']
        
        # Test 3: CUDA Streams
        if gpu_test['success']:
            streams_test = self._test_cuda_streams()
            results['tests']['cuda_streams'] = streams_test
            self.phase3_optimizations['cuda_streams'] = streams_test['success']
        
        # Test 4: VRAM Cache simulation
        if gpu_test['success']:
            cache_test = self._test_vram_cache()
            results['tests']['vram_cache'] = cache_test
            self.phase3_optimizations['vram_cache'] = cache_test['success']
        
        # Ã‰valuation globale
        successful_tests = sum(1 for test in results['tests'].values() if test['success'])
        total_tests = len(results['tests'])
        success_rate = successful_tests / total_tests if total_tests > 0 else 0
        
        results['overall_success'] = success_rate >= 0.75  # 75% tests rÃ©ussis
        results['success_rate'] = success_rate
        results['optimizations'] = self.phase3_optimizations
        
        # Log rÃ©sumÃ©
        status_emoji = "âœ…" if results['overall_success'] else "âŒ"
        self.logger.info(f"{status_emoji} INFRASTRUCTURE PHASE 3: {successful_tests}/{total_tests} tests rÃ©ussis ({success_rate:.1%})")
        
        return results
    
    def _test_gpu_infrastructure(self):
        """Test infrastructure GPU RTX 3090"""
        test = {
            'name': 'GPU Infrastructure',
            'success': False,
            'details': {}
        }
        
        try:
            self.logger.info("ğŸ® Test infrastructure GPU...")
            
            if not torch.cuda.is_available():
                test['details']['error'] = "CUDA non disponible"
                return test
            
            # Infos GPU
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            test['details'].update({
                'gpu_name': gpu_name,
                'gpu_memory_gb': gpu_memory,
                'cuda_version': torch.version.cuda
            })
            
            # VÃ©rifications
            is_rtx_3090 = "RTX 3090" in gpu_name or gpu_memory >= 20  # RTX 3090 24GB
            has_sufficient_memory = gpu_memory >= 8  # Minimum 8GB
            
            if has_sufficient_memory:
                test['success'] = True
                if is_rtx_3090 or gpu_memory >= 20:
                    self.logger.info(f"ğŸ† RTX 3090 24GB dÃ©tectÃ©: {gpu_name} ({gpu_memory:.1f}GB)")
                    self.logger.info("ğŸ’ GPU optimal pour Phase 3 - Tous avantages RTX 3090 activÃ©s")
                else:
                    self.logger.info(f"âœ… GPU compatible: {gpu_name} ({gpu_memory:.1f}GB)")
            else:
                test['details']['error'] = f"GPU {gpu_memory:.1f}GB insuffisant (minimum 8GB)"
                
        except Exception as e:
            test['details']['error'] = str(e)
            self.logger.error(f"âŒ Test GPU error: {e}")
        
        return test
    
    def _test_model_optimizer(self):
        """Test ModelOptimizer et INT8"""
        test = {
            'name': 'Model Optimizer INT8',
            'success': False,
            'details': {}
        }
        
        try:
            self.logger.info("âš¡ Test Model Optimizer...")
            
            self.model_optimizer = SimpleModelOptimizer(self.logger)
            success = self.model_optimizer.test_int8_quantification()
            
            test['success'] = success
            test['details'] = {
                'int8_enabled': self.model_optimizer.quantization_enabled,
                'fp16_model_loaded': self.model_optimizer.fp16_model is not None,
                'int8_model_loaded': self.model_optimizer.int8_model is not None
            }
            
        except Exception as e:
            test['details']['error'] = str(e)
            self.logger.error(f"âŒ Test Model Optimizer error: {e}")
        
        return test
    
    def _test_cuda_streams(self):
        """Test crÃ©ation CUDA streams"""
        test = {
            'name': 'CUDA Streams',
            'success': False,
            'details': {}
        }
        
        try:
            self.logger.info("ğŸŒŠ Test CUDA Streams...")
            
            # CrÃ©er 4 streams test
            streams = []
            for i in range(4):
                stream = torch.cuda.Stream(device=0)
                streams.append(stream)
            
            test['success'] = len(streams) == 4
            test['details'] = {
                'streams_created': len(streams),
                'target_streams': 4
            }
            
            self.logger.info(f"âœ… {len(streams)} CUDA streams crÃ©Ã©s")
            
        except Exception as e:
            test['details']['error'] = str(e)
            self.logger.error(f"âŒ Test CUDA Streams error: {e}")
        
        return test
    
    def _test_vram_cache(self):
        """Test allocation cache VRAM"""
        test = {
            'name': 'VRAM Cache 24GB',
            'success': False,
            'details': {}
        }
        
        try:
            self.logger.info("ğŸ’¾ Test VRAM Cache...")
            
            # Nettoyer cache
            torch.cuda.empty_cache()
            initial_memory = torch.cuda.memory_allocated(0) / 1024**3
            
            # Allouer cache test (1GB)
            cache_tensors = []
            cache_size_gb = 4.0 if gpu_memory >= 20 else 1.0  # 4GB cache si RTX 3090 24GB
            tensor_size = int((cache_size_gb * 1024**3) / (4 * 10))  # 10 tensors float32
            
            for i in range(10):
                tensor = torch.zeros(tensor_size // 4, dtype=torch.float32, device='cuda')
                cache_tensors.append(tensor)
            
            final_memory = torch.cuda.memory_allocated(0) / 1024**3
            allocated_gb = final_memory - initial_memory
            
            test['success'] = allocated_gb >= (2.0 if cache_size_gb >= 4.0 else 0.5)  # CritÃ¨re adaptatif RTX 3090
            test['details'] = {
                'initial_memory_gb': initial_memory,
                'final_memory_gb': final_memory,
                'allocated_gb': allocated_gb,
                'cache_tensors': len(cache_tensors)
            }
            
            self.logger.info(f"âœ… Cache VRAM: {allocated_gb:.1f}GB allouÃ©")
            
            # Cleanup
            del cache_tensors
            torch.cuda.empty_cache()
            
        except Exception as e:
            test['details']['error'] = str(e)
            self.logger.error(f"âŒ Test VRAM Cache error: {e}")
        
        return test


def main():
    """
    Test principal infrastructure Phase 3
    """
    print("ğŸš€ SuperWhisper2 Phase 3 - Test Infrastructure SimplifiÃ©")
    print("ğŸ¯ Validation: INT8 + GPU RTX 3090 + CUDA Streams + VRAM Cache")
    print()
    
    if not PHASE3_DEPS_OK:
        print("âŒ DÃ©pendances Phase 3 manquantes")
        return
    
    # CrÃ©er engine test
    engine = SimplePhase3Engine()
    
    # Test infrastructure complÃ¨te
    results = engine.test_phase3_infrastructure()
    
    # Afficher rÃ©sultats
    print("\nğŸ“Š RÃ‰SULTATS TESTS PHASE 3:")
    print(f"   SuccÃ¨s global: {'âœ…' if results['overall_success'] else 'âŒ'}")
    print(f"   Taux rÃ©ussite: {results['success_rate']:.1%}")
    print()
    
    for test_name, test_result in results['tests'].items():
        status = "âœ…" if test_result['success'] else "âŒ"
        print(f"   {status} {test_result['name']}")
        
        if 'error' in test_result['details']:
            print(f"      âŒ Erreur: {test_result['details']['error']}")
    
    print("\nğŸ¯ OPTIMISATIONS PHASE 3:")
    for opt_name, enabled in results['optimizations'].items():
        status = "âœ…" if enabled else "âŒ"
        print(f"   {status} {opt_name}")
    
    # Recommandation
    if results['overall_success']:
        print("\nğŸš€ RECOMMANDATION: Phase 3 peut continuer")
        print("   Infrastructure Phase 3 validÃ©e âœ…")
        print("   PrÃªt pour implÃ©mentation optimisations avancÃ©es")
    else:
        print("\nâš ï¸ RECOMMANDATION: Corriger infrastructure")
        print("   RÃ©soudre erreurs avant optimisations avancÃ©es")


if __name__ == "__main__":
    main() 