#!/usr/bin/env python3
"""
Script de diagnostic et correction CUDA pour SuperWhisper2
Corrige le problÃ¨me cublas64_12.dll et optimise l'installation CUDA
"""

import os
import sys
import subprocess
import json
from pathlib import Path

def check_cuda_version():
    """VÃ©rifier version CUDA installÃ©e"""
    print("ðŸ” Diagnostic CUDA...")
    
    try:
        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            print(f"âœ… NVCC trouvÃ©: {result.stdout.strip()}")
            return True
        else:
            print("âŒ NVCC non trouvÃ©")
            return False
    except FileNotFoundError:
        print("âŒ CUDA Toolkit non installÃ©")
        return False

def check_pytorch_cuda():
    """VÃ©rifier PyTorch CUDA"""
    print("\nðŸ” Diagnostic PyTorch CUDA...")
    
    try:
        import torch
        print(f"âœ… PyTorch version: {torch.__version__}")
        print(f"âœ… CUDA disponible: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"ðŸŽ® GPU dÃ©tectÃ©: {torch.cuda.get_device_name(0)}")
            print(f"ðŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
            return True
        else:
            print("âŒ CUDA non disponible dans PyTorch")
            return False
            
    except ImportError:
        print("âŒ PyTorch non installÃ©")
        return False

def check_faster_whisper():
    """VÃ©rifier faster-whisper"""
    print("\nðŸ” Diagnostic faster-whisper...")
    
    try:
        from faster_whisper import WhisperModel
        print("âœ… faster-whisper importÃ© avec succÃ¨s")
        
        # Test crÃ©ation modÃ¨le
        try:
            model = WhisperModel("tiny", device="cuda")
            print("âœ… ModÃ¨le CUDA crÃ©Ã© avec succÃ¨s")
            return True
        except Exception as e:
            print(f"âŒ Erreur crÃ©ation modÃ¨le CUDA: {e}")
            return False
            
    except ImportError as e:
        print(f"âŒ faster-whisper import error: {e}")
        return False

def check_cublas_libraries():
    """VÃ©rifier librairies cuBLAS"""
    print("\nðŸ” Diagnostic librairies cuBLAS...")
    
    # Chemins typiques Windows
    cuda_paths = [
        "C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA",
        "C:/Program Files/NVIDIA Corporation/NVSMI",
        os.environ.get('CUDA_PATH', ''),
        os.environ.get('CUDA_HOME', '')
    ]
    
    cublas_found = False
    
    for cuda_path in cuda_paths:
        if not cuda_path or not os.path.exists(cuda_path):
            continue
            
        print(f"ðŸ” Recherche dans: {cuda_path}")
        
        # Recherche rÃ©cursive des DLLs cuBLAS
        for root, dirs, files in os.walk(cuda_path):
            for file in files:
                if 'cublas' in file.lower() and file.endswith('.dll'):
                    print(f"   âœ… TrouvÃ©: {os.path.join(root, file)}")
                    cublas_found = True
    
    if not cublas_found:
        print("âŒ Aucune librairie cuBLAS trouvÃ©e")
    
    return cublas_found

def fix_cuda_installation():
    """Proposer corrections CUDA"""
    print("\nðŸ› ï¸ PROPOSITIONS DE CORRECTION")
    print("=" * 50)
    
    print("\n1. ðŸ“¥ MISE Ã€ JOUR CUDA TOOLKIT")
    print("   TÃ©lÃ©charger CUDA 12.x depuis:")
    print("   https://developer.nvidia.com/cuda-downloads")
    print("   SÃ©lectionner: Windows > x86_64 > Version > Installer")
    
    print("\n2. ðŸ”„ RÃ‰INSTALLATION PYTORCH")
    print("   Commandes Ã  exÃ©cuter:")
    print("   poetry run pip uninstall torch torchvision torchaudio")
    print("   poetry run pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
    
    print("\n3. ðŸ”§ VARIABLES D'ENVIRONNEMENT")
    print("   Ajouter au PATH:")
    print("   C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.x/bin")
    print("   C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.x/libnvvp")
    
    print("\n4. ðŸš€ FALLBACK CPU")
    print("   Si problÃ¨me persiste, utiliser CPU temporairement:")
    print("   Modifier whisper_engine_v5.py: device='cpu'")

def create_diagnostic_report():
    """CrÃ©er rapport de diagnostic"""
    print("\nðŸ“‹ CrÃ©ation rapport diagnostic...")
    
    report = {
        "timestamp": "",
        "cuda_available": False,
        "pytorch_cuda": False,
        "faster_whisper": False,
        "cublas_libraries": False,
        "recommendations": []
    }
    
    # ExÃ©cuter diagnostics
    report["cuda_available"] = check_cuda_version()
    report["pytorch_cuda"] = check_pytorch_cuda()
    report["faster_whisper"] = check_faster_whisper()
    report["cublas_libraries"] = check_cublas_libraries()
    
    # Recommandations
    if not report["cuda_available"]:
        report["recommendations"].append("Installer CUDA Toolkit 12.x")
    
    if not report["pytorch_cuda"]:
        report["recommendations"].append("RÃ©installer PyTorch avec support CUDA 12.1")
    
    if not report["cublas_libraries"]:
        report["recommendations"].append("VÃ©rifier installation cuBLAS")
    
    # Sauvegarder rapport
    with open("cuda_diagnostic_report.json", "w") as f:
        json.dump(report, f, indent=2)
    
    print("âœ… Rapport sauvegardÃ©: cuda_diagnostic_report.json")

def main():
    """Diagnostic principal"""
    print("ðŸš€ DIAGNOSTIC CUDA SUPERWHISPER2")
    print("=" * 40)
    
    # ExÃ©cuter diagnostics
    create_diagnostic_report()
    
    # Proposer corrections
    fix_cuda_installation()
    
    print("\nðŸŽ¯ PROCHAINES Ã‰TAPES:")
    print("1. Appliquer corrections CUDA")
    print("2. RedÃ©marrer systÃ¨me")
    print("3. Tester: poetry run python verify_rtx3090.py")
    print("4. Tester: poetry run python interface_test_superwhisper.py")

if __name__ == "__main__":
    main() 