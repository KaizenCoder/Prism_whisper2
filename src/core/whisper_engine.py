#!/usr/bin/env python3
"""
SuperWhisper2 Engine avec Pre-loading
Service background qui maintient le modÃ¨le Whisper en mÃ©moire RTX 3090
Optimisation: -4s latence par Ã©limination du model loading
"""

import os
import sys
import time
import threading
import queue
import traceback
from typing import Optional, Tuple
import logging

# Force utilisation GPU RTX 3090
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'

try:
    import sounddevice as sd
    import numpy as np
    from faster_whisper import WhisperModel
except ImportError as e:
    print(f"ERROR: Modules requis manquants: {e}")
    sys.exit(1)


class SuperWhisper2Engine:
    """
    Service background Whisper avec model pre-loading
    Performance: Model chargÃ© 1 fois au dÃ©marrage vs chaque transcription
    """
    
    def __init__(self):
        self.model = None
        self.model_loaded = False
        self.running = False
        
        # Queue pour requests async
        self.request_queue = queue.Queue()
        self.result_queue = queue.Queue()
        
        # Configuration audio
        self.sample_rate = 16000
        self.channels = 1
        self.duration = 3.0  # 3 secondes d'enregistrement
        
        # Setup logging
        self.logger = self._setup_logging()
        
        # Thread worker
        self.worker_thread = None
        
    def _setup_logging(self):
        """Setup logging interne"""
        logger = logging.getLogger('SuperWhisper2Engine')
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - ENGINE - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def start_engine(self):
        """DÃ©marrer le service avec pre-loading"""
        try:
            self.logger.info("ğŸš€ DÃ©marrage SuperWhisper2 Engine...")
            
            # 1. Pre-load modÃ¨le (4s une seule fois vs chaque call)
            self._preload_whisper_model()
            
            # 2. DÃ©marrer worker thread
            self.running = True
            self.worker_thread = threading.Thread(target=self._worker_loop, daemon=True)
            self.worker_thread.start()
            
            self.logger.info("âœ… SuperWhisper2 Engine prÃªt (modÃ¨le pre-loaded)")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Ã‰chec dÃ©marrage engine: {e}")
            return False
    
    def _preload_whisper_model(self):
        """Pre-loading du modÃ¨le Whisper (4s sauvÃ©s)"""
        start_time = time.time()
        self.logger.info("ğŸ“¦ Pre-loading modÃ¨le Whisper...")
        
        try:
            self.model = WhisperModel(
                "medium",
                device="cuda",
                compute_type="float16"
            )
            
            load_time = time.time() - start_time
            self.model_loaded = True
            self.logger.info(f"âœ… ModÃ¨le Whisper pre-loaded en {load_time:.1f}s")
            
            # Test dummy pour warm-up GPU
            dummy_audio = np.zeros(self.sample_rate, dtype=np.float32)
            self.model.transcribe(dummy_audio, language="fr")
            self.logger.info("ğŸ”¥ GPU warm-up terminÃ©")
            
        except Exception as e:
            self.logger.error(f"âŒ Ã‰chec pre-loading: {e}")
            self.model_loaded = False
            raise
    
    def _worker_loop(self):
        """Boucle worker pour traitement async"""
        self.logger.info("ğŸ”„ Worker loop dÃ©marrÃ©")
        
        while self.running:
            try:
                # Attendre request (timeout 1s)
                try:
                    request = self.request_queue.get(timeout=1.0)
                except queue.Empty:
                    continue
                
                # Traiter request
                if request['action'] == 'transcribe':
                    result = self._transcribe_internal()
                    self.result_queue.put({
                        'success': True,
                        'result': result,
                        'request_id': request.get('id')
                    })
                
                self.request_queue.task_done()
                
            except Exception as e:
                self.logger.error(f"âŒ Erreur worker: {e}")
                self.result_queue.put({
                    'success': False,
                    'error': str(e),
                    'request_id': request.get('id', 'unknown')
                })
    
    def transcribe_now(self, timeout=15) -> Tuple[bool, Optional[str]]:
        """
        Transcription synchrone optimisÃ©e
        Returns: (success, text)
        """
        if not self.model_loaded:
            return False, "ModÃ¨le non chargÃ©"
        
        try:
            # Request async
            request_id = f"req_{time.time()}"
            self.request_queue.put({
                'action': 'transcribe',
                'id': request_id
            })
            
            # Attendre rÃ©sultat
            start_time = time.time()
            while time.time() - start_time < timeout:
                try:
                    result = self.result_queue.get(timeout=0.5)
                    if result.get('request_id') == request_id:
                        if result['success']:
                            return True, result['result']
                        else:
                            return False, result.get('error', 'Erreur inconnue')
                except queue.Empty:
                    continue
            
            return False, "Timeout transcription"
            
        except Exception as e:
            return False, f"Erreur transcription: {e}"
    
    def _transcribe_internal(self) -> Optional[str]:
        """Transcription interne optimisÃ©e (modÃ¨le dÃ©jÃ  chargÃ©)"""
        try:
            # 1. Enregistrement audio (3s)
            self.logger.info("ğŸ¤ Enregistrement audio...")
            audio_data = sd.rec(
                frames=int(self.duration * self.sample_rate),
                samplerate=self.sample_rate,
                channels=self.channels,
                dtype=np.float32
            )
            sd.wait()
            
            # 2. Transcription (modÃ¨le dÃ©jÃ  en mÃ©moire = ultra-rapide)
            self.logger.info("ğŸ§  Transcription...")
            audio_flat = audio_data.flatten().astype(np.float32)
            
            segments, info = self.model.transcribe(
                audio_flat,
                language="fr",
                condition_on_previous_text=False
            )
            
            # 3. Collecte rÃ©sultats
            text_parts = []
            for segment in segments:
                text = segment.text.strip()
                if text:
                    text_parts.append(text)
            
            if text_parts:
                result = " ".join(text_parts)
                self.logger.info(f"âœ… Transcription: {result[:50]}...")
                return result
            else:
                self.logger.info("âš ï¸ Aucun audio dÃ©tectÃ©")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ Erreur transcription interne: {e}")
            traceback.print_exc()
            return None
    
    def get_status(self) -> dict:
        """Status du service"""
        return {
            'running': self.running,
            'model_loaded': self.model_loaded,
            'queue_size': self.request_queue.qsize(),
            'memory_info': self._get_gpu_memory() if self.model_loaded else None
        }
    
    def _get_gpu_memory(self) -> dict:
        """Info mÃ©moire GPU RTX 3090"""
        try:
            import torch
            if torch.cuda.is_available():
                return {
                    'allocated': f"{torch.cuda.memory_allocated(0) / 1024**3:.1f}GB",
                    'reserved': f"{torch.cuda.memory_reserved(0) / 1024**3:.1f}GB"
                }
        except:
            pass
        return {'status': 'unavailable'}
    
    def stop_engine(self):
        """ArrÃªter le service proprement"""
        self.logger.info("ğŸ›‘ ArrÃªt SuperWhisper2 Engine...")
        self.running = False
        
        if self.worker_thread and self.worker_thread.is_alive():
            self.worker_thread.join(timeout=5)
        
        # Cleanup GPU memory
        if self.model_loaded:
            try:
                del self.model
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                self.logger.info("ğŸ§¹ MÃ©moire GPU nettoyÃ©e")
            except:
                pass
        
        self.logger.info("âœ… Engine arrÃªtÃ©")


# Instance globale (singleton pattern)
_engine_instance = None

def get_engine() -> SuperWhisper2Engine:
    """Obtenir l'instance globale du service"""
    global _engine_instance
    if _engine_instance is None:
        _engine_instance = SuperWhisper2Engine()
    return _engine_instance


def start_service():
    """DÃ©marrer le service background"""
    engine = get_engine()
    return engine.start_engine()


def transcribe() -> Tuple[bool, Optional[str]]:
    """Interface simple pour transcription"""
    engine = get_engine()
    return engine.transcribe_now()


if __name__ == "__main__":
    # Test standalone
    print("ğŸ§ª Test SuperWhisper2 Engine...")
    
    engine = SuperWhisper2Engine()
    if engine.start_engine():
        print("âœ… Service dÃ©marrÃ©, test transcription...")
        success, result = engine.transcribe_now()
        if success:
            print(f"âœ… Transcription: {result}")
        else:
            print(f"âŒ Ã‰chec: {result}")
        
        print(f"ğŸ“Š Status: {engine.get_status()}")
        engine.stop_engine()
    else:
        print("âŒ Ã‰chec dÃ©marrage service") 