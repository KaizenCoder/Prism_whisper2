#!/usr/bin/env python3
"""
Audio Streamer Asynchrone pour Prism_whisper2
ParallÃ©lisation capture audio + transcription pour -1s latence
Architecture: Buffer circulaire + VAD + Pipeline async
"""

import numpy as np
import sounddevice as sd
import threading
import queue
import time
import logging
from typing import Optional, Callable, List, Tuple
from collections import deque
import wave
import tempfile
import webrtcvad
import struct
from scipy import signal


class VoiceActivityDetector:
    """
    DÃ©tecteur d'activitÃ© vocale pour Ã©liminer les hallucinations Whisper
    """
    def __init__(self, sample_rate=16000, aggressiveness=1):  # Moins agressif: 2â†’1
        self.sample_rate = sample_rate
        self.vad = webrtcvad.Vad()
        self.vad.set_mode(aggressiveness)  # 0-3, 3 = plus agressif
        
        # Seuils de dÃ©tection ASSOUPLIS pour E3
        self.min_voice_ratio = 0.1  # 10% du chunk (vs 30% trop strict)
        self.min_energy_threshold = 0.0005  # Seuil plus bas (vs 0.001 trop haut)
        
    def has_voice_activity(self, audio_chunk: np.ndarray) -> bool:
        """
        DÃ©termine si le chunk audio contient de la vraie parole
        """
        # 1. VÃ©rifier Ã©nergie minimum
        energy = np.mean(audio_chunk ** 2)
        if energy < self.min_energy_threshold:
            return False
            
        # 2. WebRTC VAD sur segments de 30ms
        frame_length = int(self.sample_rate * 0.03)  # 30ms frames pour WebRTC VAD
        voice_frames = 0
        total_frames = 0
        
        # Convertir float32 â†’ int16 pour WebRTC VAD  
        audio_int16 = (audio_chunk * 32767).astype(np.int16)
        
        for i in range(0, len(audio_int16) - frame_length, frame_length):
            frame = audio_int16[i:i + frame_length]
            
            if len(frame) == frame_length:
                # WebRTC VAD nÃ©cessite bytes
                frame_bytes = struct.pack(f'{len(frame)}h', *frame)
                
                try:
                    if self.vad.is_speech(frame_bytes, self.sample_rate):
                        voice_frames += 1
                    total_frames += 1
                except:
                    # Fallback si WebRTC VAD Ã©choue
                    continue
        
        if total_frames == 0:
            return False
            
        voice_ratio = voice_frames / total_frames
        return voice_ratio >= self.min_voice_ratio
    
    def filter_noise(self, audio_chunk: np.ndarray) -> np.ndarray:
        """
        Filtre simple pour rÃ©duire le bruit de fond
        """
        # Filtre passe-haut pour Ã©liminer les basses frÃ©quences (bruit Ã©lectronique)
        nyquist = self.sample_rate / 2
        high_cutoff = 300 / nyquist  # 300Hz
        b, a = signal.butter(4, high_cutoff, btype='high')
        filtered = signal.filtfilt(b, a, audio_chunk)
        
        # Normalisation douce
        max_val = np.max(np.abs(filtered))
        if max_val > 0:
            filtered = filtered / max_val * 0.8
            
        return filtered.astype(np.float32)


class HallucinationFilter:
    """
    Filtre pour dÃ©tecter et Ã©liminer les hallucinations communes de Whisper
    """
    def __init__(self):
        # Phrases d'hallucination communes identifiÃ©es
        self.hallucination_patterns = [
            "sous-titres rÃ©alisÃ©s par la communautÃ© d'amara.org",
            "sous-titres rÃ©alisÃ©s par l'amara.org", 
            "merci d'avoir regardÃ© cette vidÃ©o",
            "merci d'avoir regardÃ©",
            "n'hÃ©sitez pas Ã  vous abonner",
            "like et abonne-toi",
            "commentez et partagez",
            "Ã  bientÃ´t pour une nouvelle vidÃ©o",
            "musique libre de droit",
            "copyright",
            "creative commons"
        ]
        
    def is_hallucination(self, text: str) -> bool:
        """
        DÃ©termine si le texte est probablement une hallucination
        """
        if not text or len(text.strip()) == 0:
            return True
            
        text_lower = text.lower().strip()
        
        # VÃ©rifier patterns d'hallucination
        for pattern in self.hallucination_patterns:
            if pattern in text_lower:
                return True
                
        # VÃ©rifier rÃ©pÃ©titions suspectes
        words = text_lower.split()
        if len(words) > 3:
            unique_ratio = len(set(words)) / len(words)
            if unique_ratio < 0.5:  # Trop de rÃ©pÃ©titions
                return True
                
        return False


class AudioStreamer:
    """
    Capture l'audio du microphone en continu et le transmet via un callback.
    IntÃ¨gre VAD et filtrage anti-hallucination.
    """
    def __init__(self, callback: Callable[[np.ndarray], None], logger: any, sample_rate=16000, chunk_duration=3.0, device_name="Rode NT-USB"):
        self.sample_rate = sample_rate
        self.chunk_duration = chunk_duration
        self.chunk_frames = int(sample_rate * chunk_duration)
        self.channels = 1
        self.device_name = device_name
        
        self.callback = callback
        self.logger = logger
        
        # RÃ©solution automatique du pÃ©riphÃ©rique par nom
        self.device_id = self._resolve_device_id(self.device_name)
        
        # IntÃ©gration VAD et filtres
        self.vad = VoiceActivityDetector(sample_rate)
        self.hallucination_filter = HallucinationFilter()
        
        self.running = False
        self.stream = None
        self.thread = None
        
        # Stats de filtrage
        self.stats = {
            'chunks_processed': 0,
            'chunks_with_voice': 0,
            'chunks_filtered_noise': 0,
            'hallucinations_detected': 0
        }

    def _resolve_device_id(self, name_part: str) -> int:
        """
        Trouve l'ID du pÃ©riphÃ©rique audio dont le nom contient name_part.
        Robuste aux changements d'ID Windows lors branchement/dÃ©branchement.
        """
        try:
            devices = sd.query_devices()
            for idx, device in enumerate(devices):
                device_name = device.get('name', '').lower()
                max_input_channels = device.get('max_input_channels', 0)
                
                if name_part.lower() in device_name and max_input_channels > 0:
                    self.logger.info(f"ğŸ¤ PÃ©riphÃ©rique audio dÃ©tectÃ©: ID {idx} - {device['name']}")
                    return idx
                    
        except Exception as e:
            self.logger.warning(f"âš ï¸ Erreur lors de la recherche des pÃ©riphÃ©riques: {e}")
        
        # Fallback: utiliser pÃ©riphÃ©rique par dÃ©faut avec warning
        self.logger.warning(f"âŒ PÃ©riphÃ©rique '{name_part}' non trouvÃ©. Utilisation du pÃ©riphÃ©rique par dÃ©faut.")
        return None  # sd.InputStream utilisera le device par dÃ©faut

    def start(self):
        """DÃ©marre la capture audio."""
        if self.running:
            return
        self.logger.info("ğŸ¤ DÃ©marrage de la capture audio avec VAD...")
        self.running = True
        self.thread = threading.Thread(target=self._capture_loop)
        self.thread.start()

    def _capture_loop(self):
        """Boucle de capture qui lit depuis le microphone."""
        # âœ… CORRECTIF CRITIQUE: Forcer device= pour capturer depuis Rode NT-USB
        self.stream = sd.InputStream(
            device=self.device_id,  # â† FIX MAJEUR: route vers le bon micro
            samplerate=self.sample_rate,
            channels=self.channels,
            dtype=np.float32,
            blocksize=self.chunk_frames,
            callback=self._audio_callback
        )
        with self.stream:
            while self.running:
                time.sleep(0.1) # La boucle est principalement pilotÃ©e par le callback

    def _audio_callback(self, indata, frames, time_info, status):
        """Callback de sounddevice, appelÃ© avec de nouvelles donnÃ©es audio."""
        if status:
            self.logger.warning(f"AudioStreamer status: {status}")
            
        if not self.running or not self.callback:
            return
            
        try:
            audio_chunk = indata[:, 0]
            self.stats['chunks_processed'] += 1
            
            # 1. VÃ©rifier activitÃ© vocale avec VAD (E3: mode permissif)
            if not self.vad.has_voice_activity(audio_chunk):
                self.stats['chunks_filtered_noise'] += 1
                # E3 DEBUG: Log les dÃ©tails du filtrage pour diagnostic
                energy = np.mean(audio_chunk ** 2)
                self.logger.debug(f"ğŸ”‡ Chunk filtrÃ© - Ã‰nergie: {energy:.6f}, Seuil: {self.vad.min_energy_threshold:.6f}")
                return
                
            self.stats['chunks_with_voice'] += 1
            
            # 2. Filtrer le bruit
            filtered_audio = self.vad.filter_noise(audio_chunk)
            
            # 3. Transmettre Ã  l'engine avec callback wrappÃ©
            self._safe_callback(filtered_audio)
            
        except Exception as e:
            self.logger.error(f"Erreur dans le callback AudioStreamer: {e}")

    def _safe_callback(self, audio_chunk: np.ndarray):
        """
        Callback sÃ©curisÃ© qui wrappe le callback original avec filtrage des hallucinations
        """
        # Wrapper le callback original pour intercepter les transcriptions
        original_callback = self.callback
        
        def wrapped_transcription_callback(text: str):
            """Callback interceptÃ© pour filtrer les hallucinations"""
            if self.hallucination_filter.is_hallucination(text):
                self.stats['hallucinations_detected'] += 1
                self.logger.warning(f"ğŸš« Hallucination dÃ©tectÃ©e et filtrÃ©e: '{text[:50]}...'")
                return
                
            # Transcription valide - la transmettre
            if hasattr(self, '_original_transcription_callback'):
                self._original_transcription_callback(text)
        
        # Appeler callback avec l'audio filtrÃ©
        original_callback(audio_chunk)

    def stop(self):
        """ArrÃªte la capture audio."""
        if not self.running:
            return
        
        self.logger.info("ğŸ›‘ ArrÃªt de la capture audio...")
        self.logger.info(f"ğŸ“Š Stats filtrage: {self.stats}")
        
        self.running = False
        if self.thread:
            self.thread.join(timeout=2)
        if self.stream:
            self.stream.close()
        self.logger.info("âœ… Capture audio arrÃªtÃ©e.")

    def get_filtering_stats(self) -> dict:
        """Retourne les statistiques de filtrage"""
        return self.stats.copy()

    def add_to_buffer(self, audio_data: np.ndarray):
        """
        MÃ©thode pour ajouter manuellement des donnÃ©es audio au flux.
        Utile pour les tests. Simule une capture micro.
        """
        if self.running and self.callback:
            self.logger.info(f"Injecting {len(audio_data)/self.sample_rate:.2f}s of audio for testing.")
            self.callback(audio_data)


class AudioStreamingManager:
    """
    Chef d'orchestre pour le streaming audio continu et la transcription
    """
    def __init__(self, whisper_engine, device_name="Rode NT-USB"):
        self.engine = whisper_engine
        self.logger = self.engine.logger
        self.streamer = AudioStreamer(
            callback=self.engine.process_audio_chunk,
            logger=self.logger,
            sample_rate=self.engine.sample_rate,
            chunk_duration=1.0,  # Traiter l'audio par chunks de 1s
            device_name=device_name  # âœ… Passage du device name
        )
        self.continuous_mode = False
        self.last_result = None

    def _handle_transcription(self, audio_data: np.ndarray):
        """
        GÃ¨re la logique de transcription d'un chunk audio.
        Cette mÃ©thode est maintenant directement dans l'engine (process_audio_chunk)
        pour un couplage plus propre.
        """
        # obsolÃ¨te
        pass

    def start_continuous_mode(self) -> bool:
        """DÃ©marrer mode streaming continu"""
        if not self.engine.model_loaded:
            self.logger.error("âŒ Engine Whisper pas prÃªt")
            return False
        
        self.logger.info("ğŸŒŠ DÃ©marrage mode streaming continu...")
        self.continuous_mode = True
        return self.streamer.start()
    
    def get_latest_result(self, timeout=1.0) -> Optional[dict]:
        """RÃ©cupÃ©rer dernier rÃ©sultat transcription"""
        try:
            return self.results_queue.get(timeout=timeout)
        except queue.Empty:
            return None
    
    def stop_continuous_mode(self):
        """ArrÃªter mode streaming"""
        self.logger.info("â¹ï¸ ArrÃªt mode streaming continu...")
        self.continuous_mode = False
        self.streamer.stop()
    
    def get_stats(self) -> dict:
        """Stats complÃ¨tes streaming"""
        stats = {
            'manager_active': self.continuous_mode,
            'results_pending': self.results_queue.qsize(),
            'whisper_ready': self.engine.model_loaded
        }
        return stats


if __name__ == "__main__":
    # Test standalone streaming
    import sys
    sys.path.append('..')
    
    print("ğŸ§ª Test Audio Streaming Asynchrone...")
    
    def dummy_transcription(audio_data):
        print(f"ğŸ“ Transcription simulÃ©e: {len(audio_data)} samples")
        time.sleep(1)  # Simuler processing
        print("âœ… Transcription terminÃ©e")
    
    streamer = AudioStreamer(dummy_transcription, logging.getLogger('AudioStreamer'))
    
    if streamer.start():
        print("âœ… Streaming dÃ©marrÃ©, test 10s...")
        time.sleep(10)
        
        stats = streamer.get_filtering_stats()
        print(f"ğŸ“Š Stats: {stats}")
        
        streamer.stop()
    else:
        print("âŒ Ã‰chec streaming") 