#!/usr/bin/env python3
"""
Audio Streamer Asynchrone pour Prism_whisper2
Parall√©lisation capture audio + transcription pour -1s latence
Architecture: Buffer circulaire + VAD + Pipeline async
"""

import numpy as np
import sounddevice as sd
import threading
import queue
import time
import logging
from typing import Optional, Callable, List, Tuple
from collections import deque
import wave
import tempfile
import webrtcvad
import struct
from scipy import signal


class VoiceActivityDetector:
    """
    D√©tecteur d'activit√© vocale pour √©liminer les hallucinations Whisper
    """
    def __init__(self, sample_rate=16000, aggressiveness=1):  # Moins agressif: 2‚Üí1
        self.sample_rate = sample_rate
        self.vad = webrtcvad.Vad()
        self.vad.set_mode(aggressiveness)  # 0-3, 3 = plus agressif
        
        # Seuils de d√©tection ASSOUPLIS pour E3
        self.min_voice_ratio = 0.1  # 10% du chunk (vs 30% trop strict)
        self.min_energy_threshold = 0.0005  # Seuil plus bas (vs 0.001 trop haut)
        
    def has_voice_activity(self, audio_chunk: np.ndarray) -> bool:
        """
        D√©termine si le chunk audio contient de la vraie parole
        """
        # 1. V√©rifier √©nergie minimum
        energy = np.mean(audio_chunk ** 2)
        if energy < self.min_energy_threshold:
            return False
            
        # 2. WebRTC VAD sur segments de 30ms
        frame_length = int(self.sample_rate * 0.03)  # 30ms frames pour WebRTC VAD
        voice_frames = 0
        total_frames = 0
        
        # Convertir float32 ‚Üí int16 pour WebRTC VAD  
        audio_int16 = (audio_chunk * 32767).astype(np.int16)
        
        for i in range(0, len(audio_int16) - frame_length, frame_length):
            frame = audio_int16[i:i + frame_length]
            
            if len(frame) == frame_length:
                # WebRTC VAD n√©cessite bytes
                frame_bytes = struct.pack(f'{len(frame)}h', *frame)
                
                try:
                    if self.vad.is_speech(frame_bytes, self.sample_rate):
                        voice_frames += 1
                    total_frames += 1
                except:
                    # Fallback si WebRTC VAD √©choue
                    continue
        
        if total_frames == 0:
            return False
            
        voice_ratio = voice_frames / total_frames
        return voice_ratio >= self.min_voice_ratio
    
    def filter_noise(self, audio_chunk: np.ndarray) -> np.ndarray:
        """
        Filtre simple pour r√©duire le bruit de fond
        """
        # Filtre passe-haut pour √©liminer les basses fr√©quences (bruit √©lectronique)
        nyquist = self.sample_rate / 2
        high_cutoff = 300 / nyquist  # 300Hz
        b, a = signal.butter(4, high_cutoff, btype='high')
        filtered = signal.filtfilt(b, a, audio_chunk)
        
        # Normalisation douce
        max_val = np.max(np.abs(filtered))
        if max_val > 0:
            filtered = filtered / max_val * 0.8
            
        return filtered.astype(np.float32)


class HallucinationFilter:
    """
    Filtre pour d√©tecter et √©liminer les hallucinations communes de Whisper
    """
    def __init__(self):
        # Phrases d'hallucination communes identifi√©es
        self.hallucination_patterns = [
            "sous-titres r√©alis√©s par la communaut√© d'amara.org",
            "sous-titres r√©alis√©s par l'amara.org", 
            "merci d'avoir regard√© cette vid√©o",
            "merci d'avoir regard√©",
            "n'h√©sitez pas √† vous abonner",
            "like et abonne-toi",
            "commentez et partagez",
            "√† bient√¥t pour une nouvelle vid√©o",
            "musique libre de droit",
            "copyright",
            "creative commons"
        ]
        
    def is_hallucination(self, text: str) -> bool:
        """
        D√©termine si le texte est probablement une hallucination
        """
        if not text or len(text.strip()) == 0:
            return True
            
        text_lower = text.lower().strip()
        
        # V√©rifier patterns d'hallucination
        for pattern in self.hallucination_patterns:
            if pattern in text_lower:
                return True
                
        # V√©rifier r√©p√©titions suspectes
        words = text_lower.split()
        if len(words) > 3:
            unique_ratio = len(set(words)) / len(words)
            if unique_ratio < 0.5:  # Trop de r√©p√©titions
                return True
                
        return False


class AudioStreamer:
    """
    Capture l'audio du microphone en continu et le transmet via un callback.
    Int√®gre VAD et filtrage anti-hallucination.
    """
    def __init__(self, callback: Callable[[np.ndarray], None], logger: any, sample_rate=16000, chunk_duration=3.0, device_name="Rode NT-USB"):
        self.sample_rate = sample_rate
        self.chunk_duration = chunk_duration
        self.chunk_frames = int(sample_rate * chunk_duration)
        self.channels = 1
        self.device_name = device_name
        
        self.callback = callback
        self.logger = logger
        
        # R√©solution automatique du p√©riph√©rique par nom
        self.device_id = self._resolve_device_id(self.device_name)
        
        # Int√©gration VAD et filtres
        self.vad = VoiceActivityDetector(sample_rate)
        self.hallucination_filter = HallucinationFilter()
        
        self.running = False
        self.stream = None
        self.thread = None
        
        # Stats de filtrage
        self.stats = {
            'chunks_processed': 0,
            'chunks_with_voice': 0,
            'chunks_filtered_noise': 0,
            'hallucinations_detected': 0
        }

    def _resolve_device_id(self, name_part: str) -> int:
        """
        Trouve l'ID du p√©riph√©rique audio dont le nom contient name_part.
        Robuste aux changements d'ID Windows lors branchement/d√©branchement.
        """
        try:
            devices = sd.query_devices()
            for idx, device in enumerate(devices):
                device_name = device.get('name', '').lower()
                max_input_channels = device.get('max_input_channels', 0)
                
                if name_part.lower() in device_name and max_input_channels > 0:
                    self.logger.info(f"üé§ P√©riph√©rique audio d√©tect√©: ID {idx} - {device['name']}")
                    return idx
                    
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Erreur lors de la recherche des p√©riph√©riques: {e}")
        
        # Fallback: utiliser p√©riph√©rique par d√©faut avec warning
        self.logger.warning(f"‚ùå P√©riph√©rique '{name_part}' non trouv√©. Utilisation du p√©riph√©rique par d√©faut.")
        return None  # sd.InputStream utilisera le device par d√©faut

    def start(self):
        """D√©marre la capture audio."""
        if self.running:
            return
        self.logger.info("üé§ D√©marrage de la capture audio avec VAD...")
        self.running = True
        self.thread = threading.Thread(target=self._capture_loop)
        self.thread.start()

    def _capture_loop(self):
        """Boucle de capture qui lit depuis le microphone."""
        # ‚úÖ CORRECTIF CRITIQUE: Forcer device= pour capturer depuis Rode NT-USB
        self.stream = sd.InputStream(
            device=self.device_id,  # ‚Üê FIX MAJEUR: route vers le bon micro
            samplerate=self.sample_rate,
            channels=self.channels,
            dtype=np.float32,
            blocksize=self.chunk_frames,
            callback=self._audio_callback
        )
        with self.stream:
            while self.running:
                time.sleep(0.1) # La boucle est principalement pilot√©e par le callback

    def _audio_callback(self, indata, frames, time_info, status):
        """Callback de sounddevice, appel√© avec de nouvelles donn√©es audio."""
        if status:
            self.logger.warning(f"AudioStreamer status: {status}")
            
        if not self.running or not self.callback:
            return
            
        try:
            audio_chunk = indata[:, 0]
            self.stats['chunks_processed'] += 1
            
            # 1. V√©rifier activit√© vocale avec VAD (E3: mode permissif)
            if not self.vad.has_voice_activity(audio_chunk):
                self.stats['chunks_filtered_noise'] += 1
                # E3 DEBUG: Log les d√©tails du filtrage pour diagnostic
                energy = np.mean(audio_chunk ** 2)
                self.logger.debug(f"üîá Chunk filtr√© - √ânergie: {energy:.6f}, Seuil: {self.vad.min_energy_threshold:.6f}")
                return
                
            self.stats['chunks_with_voice'] += 1
            
            # 2. Filtrer le bruit
            filtered_audio = self.vad.filter_noise(audio_chunk)
            
            # 3. Transmettre √† l'engine avec callback wrapp√©
            self._safe_callback(filtered_audio)
            
        except Exception as e:
            self.logger.error(f"Erreur dans le callback AudioStreamer: {e}")

    def _safe_callback(self, audio_chunk: np.ndarray):
        """
        Callback s√©curis√© qui wrappe le callback original avec filtrage des hallucinations
        """
        # Wrapper le callback original pour intercepter les transcriptions
        original_callback = self.callback
        
        def wrapped_transcription_callback(text: str):
            """Callback intercept√© pour filtrer les hallucinations"""
            if self.hallucination_filter.is_hallucination(text):
                self.stats['hallucinations_detected'] += 1
                self.logger.warning(f"üö´ Hallucination d√©tect√©e et filtr√©e: '{text[:50]}...'")
                return
                
            # Transcription valide - la transmettre
            if hasattr(self, '_original_transcription_callback'):
                self._original_transcription_callback(text)
        
        # Appeler callback avec l'audio filtr√©
        original_callback(audio_chunk)

    def stop(self):
        """Arr√™te la capture audio."""
        if not self.running:
            return
        
        self.logger.info("üõë Arr√™t de la capture audio...")
        self.logger.info(f"üìä Stats filtrage: {self.stats}")
        
        self.running = False
        if self.thread:
            self.thread.join(timeout=2)
        if self.stream:
            self.stream.close()
        self.logger.info("‚úÖ Capture audio arr√™t√©e.")

    def get_filtering_stats(self) -> dict:
        """Retourne les statistiques de filtrage"""
        return self.stats.copy()

    def add_to_buffer(self, audio_data: np.ndarray):
        """
        M√©thode pour ajouter manuellement des donn√©es audio au flux.
        Utile pour les tests. Simule une capture micro.
        """
        if self.running and self.callback:
            self.logger.info(f"Injecting {len(audio_data)/self.sample_rate:.2f}s of audio for testing.")
            self.callback(audio_data)


class AudioStreamingManager:
    """
    Chef d'orchestre pour le streaming audio continu et la transcription
    """
    def __init__(self, whisper_engine, device_name="Rode NT-USB"):
        self.engine = whisper_engine
        self.logger = self.engine.logger
        self.streamer = AudioStreamer(
            callback=self.engine.process_audio_chunk,
            logger=self.logger,
            sample_rate=self.engine.sample_rate,
            chunk_duration=1.0,  # Traiter l'audio par chunks de 1s
            device_name=device_name  # ‚úÖ Passage du device name
        )
        self.continuous_mode = False
        self.last_result = None

    def _handle_transcription(self, audio_data: np.ndarray):
        """
        G√®re la logique de transcription d'un chunk audio.
        Cette m√©thode est maintenant directement dans l'engine (process_audio_chunk)
        pour un couplage plus propre.
        """
        # obsol√®te
        pass

    def start_continuous_mode(self) -> bool:
        """D√©marrer mode streaming continu"""
        if not self.engine.model_loaded:
            self.logger.error("‚ùå Engine Whisper pas pr√™t")
            return False
        
        self.logger.info("üåä D√©marrage mode streaming continu...")
        self.continuous_mode = True
        return self.streamer.start()
    
    def get_latest_result(self, timeout=1.0) -> Optional[dict]:
        """R√©cup√©rer dernier r√©sultat transcription"""
        try:
            return self.results_queue.get(timeout=timeout)
        except queue.Empty:
            return None
    
    def stop_continuous_mode(self):
        """Arr√™ter mode streaming"""
        self.logger.info("‚èπÔ∏è Arr√™t mode streaming continu...")
        self.continuous_mode = False
        self.streamer.stop()
    
    def get_stats(self) -> dict:
        """Stats compl√®tes streaming"""
        stats = {
            'manager_active': self.continuous_mode,
            'results_pending': self.results_queue.qsize(),
            'whisper_ready': self.engine.model_loaded
        }
        return stats


if __name__ == "__main__":
    # Test standalone streaming
    import sys
    sys.path.append('..')
    
    print("üß™ Test Audio Streaming Asynchrone...")
    
    def dummy_transcription(audio_data):
        print(f"üìù Transcription simul√©e: {len(audio_data)} samples")
        time.sleep(1)  # Simuler processing
        print("‚úÖ Transcription termin√©e")
    
    streamer = AudioStreamer(dummy_transcription, logging.getLogger('AudioStreamer'))
    
    if streamer.start():
        print("‚úÖ Streaming d√©marr√©, test 10s...")
        time.sleep(10)
        
        stats = streamer.get_filtering_stats()
        print(f"üìä Stats: {stats}")
        
        streamer.stop()
    else:
        print("‚ùå √âchec streaming") 